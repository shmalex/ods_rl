{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566d23a0",
   "metadata": {},
   "source": [
    "## Home Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8d582d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import Frozen_Lake as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cbdd1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def init_policy(env):\n",
    "\n",
    "    actions_set = set()\n",
    "\n",
    "    row_max = 0\n",
    "    col_max = 0\n",
    "    for state in env.get_all_states():\n",
    "        actions_set.update(env.get_possible_actions(state))\n",
    "        row_max = max(row_max, state[0])\n",
    "        col_max = max(col_max, state[1])\n",
    "\n",
    "    actions = {i:action for i,action in enumerate(sorted(actions_set))}\n",
    "\n",
    "    policy = np.zeros((row_max+1, col_max+1, len(actions_set)))\n",
    "\n",
    "#     print('policies    |states')\n",
    "    for y,x in env.get_all_states():\n",
    "        possible_actions = env.get_possible_actions((y,x))\n",
    "        if len(possible_actions)!=0:\n",
    "            uniform_prob = 1/len(possible_actions)\n",
    "            policy[y][x] = uniform_prob\n",
    "#         print(policy[y][x], y, x,)\n",
    "    return policy, list(sorted(actions_set))\n",
    "\n",
    "def init_values(policy):\n",
    "     return np.zeros((policy.shape[0]*policy.shape[1],))\n",
    "\n",
    "def value_iteration(value, gamma):\n",
    "    new_values = np.zeros(policy.shape[0]*policy.shape[1])\n",
    "    for state in env.get_all_states():\n",
    "        state_y, state_x = state\n",
    "        idx_state_values = state_y*policy.shape[0] + state_x\n",
    "        actions = []\n",
    "        for i, action in enumerate(env.get_possible_actions(state)):\n",
    "            state_action = 0\n",
    "            policy_prob = policy[state_y][state_x][actions_dict[action]]\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                next_state_y, next_state_x = next_state\n",
    "                idx_next_state_values = next_state_y*policy.shape[0] + next_state_x\n",
    "                # reward\n",
    "                reward = env.get_reward(state, action, next_state)\n",
    "                # value\n",
    "                trans_prob = env.get_transition_prob(state, action, next_state)\n",
    "                next_value = values[idx_next_state_values]\n",
    "                state_action += policy_prob * trans_prob * (reward + gamma * next_value)\n",
    "            actions.append(state_action)\n",
    "        if len(actions)!=0:\n",
    "            new_values[idx_state_values] = max(actions)\n",
    "    \n",
    "    return new_values\n",
    "def policy_evalu__ation_step(policy, values, gamma):\n",
    "    new_values = np.zeros(policy.shape[0]*policy.shape[1])\n",
    "\n",
    "    for state in env.get_all_states():\n",
    "        state_y, state_x = state\n",
    "        idx_state_values = state_y*policy.shape[0] + state_x\n",
    "        for action in env.get_possible_actions(state):\n",
    "            policy_prob = policy[state_y][state_x][actions_dict[action]]\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                next_state_y, next_state_x = next_state\n",
    "                idx_next_state_values = next_state_y*policy.shape[0] + next_state_x\n",
    "\n",
    "                # reward\n",
    "                reward = env.get_reward(state, action, next_state)\n",
    "                # value\n",
    "                trans_prob = env.get_transition_prob(state, action, next_state)\n",
    "                next_value = values[idx_next_state_values]\n",
    "                new_values[idx_state_values] += \\\n",
    "                    policy_prob * trans_prob * (reward + gamma * next_value)\n",
    "\n",
    "    return new_values\n",
    "\n",
    "def Q(policy, values, actions_dict):\n",
    "    new_q = np.zeros(policy.shape)\n",
    "\n",
    "    for state in env.get_all_states():\n",
    "        state_y, state_x = state\n",
    "        # actions_dict[action]\n",
    "        for action in env.get_possible_actions(state):\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                next_state_y, next_state_x = next_state\n",
    "\n",
    "                reward = env.get_reward(state, action, next_state)\n",
    "                prob = env.get_transition_prob(state, action, next_state)\n",
    "                next_value = values[next_state_y*policy.shape[0]+next_state_x]\n",
    "\n",
    "                new_q[state_y][state_x][actions_dict[action]] \\\n",
    "                    += prob *(reward + gamma * next_value)\n",
    "    return new_q\n",
    "\n",
    "def policy_improvement(policy, q):\n",
    "    next_policy = np.zeros(policy.shape)\n",
    "    for state in env.get_all_states():\n",
    "        state_y,state_x = state\n",
    "\n",
    "        idx_action = np.argmax(q[state_y][state_x])\n",
    "        next_policy[state_y][state_x][idx_action] = 1\n",
    "    return next_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0f0407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = fl.FrozenLakeEnv(map_name=\"4x4\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f67a6a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['down', 'left', 'right', 'up'], {'down': 0, 'left': 1, 'right': 2, 'up': 3})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy,actions_set = init_policy(env)\n",
    "actions_dict ={action:i for i,action in enumerate(sorted(actions_set))}\n",
    "actions_arr = [0]*4\n",
    "for k,v in actions_dict.items():\n",
    "    actions_arr[v]=k\n",
    "actions_arr, actions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "594c6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = 100\n",
    "K = 100\n",
    "gamma = 0.8\n",
    "\n",
    "values = init_values(policy)\n",
    "for k in tqdm.tqdm(range(K), position=0):\n",
    "    for l in range(L):\n",
    "        values = value_iteration(values, gamma)\n",
    "    q = Q(policy, values, actions_dict)\n",
    "    policy = policy_improvement(policy, q)\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf632e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(policy, env, actions_arr):\n",
    "    field = []\n",
    "    row = []\n",
    "    for y, x in env.get_all_states():\n",
    "        row.append(actions_arr[np.argmax(policy[x][y])][0])\n",
    "        if x == 3:\n",
    "            field.append(' '.join(row))\n",
    "            row = []\n",
    "    print('\\n'.join(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c397bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d d r d\n",
      "r d d r\n",
      "d d d r\n",
      "l d d d\n"
     ]
    }
   ],
   "source": [
    "render_policy(policy, env, actions_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52c8487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(100):\n",
    "        possible_actions = list(sorted(env.get_possible_actions(state)))\n",
    "        state_y, state_x = state\n",
    "        actions_distr = policy[state_y][state_x]\n",
    "\n",
    "        action = np.random.choice(possible_actions,p=actions_distr)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         print(state, reward, done, _,action, actions_distr)\n",
    "#         break\n",
    "#         env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        if done: break\n",
    "    return total_reward, step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d52299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(g, r, s):\n",
    "    sucess = np.sum(r[r==1])/len(r)\n",
    "    fail = 1 - sucess\n",
    "    sucess_len = np.mean(s[r==1])\n",
    "    fail_len = np.mean(s[r==0])\n",
    "    return g, sucess, fail, sucess_len, fail_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d8e2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_test(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3295bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MDP(L, K, gamma, tqdm_train):\n",
    "    tqdm_train.reset()\n",
    "    policy,actions_set = init_policy(env)\n",
    "    actions_dict ={action:i for i,action in enumerate(sorted(actions_set))}\n",
    "    values = init_values(policy)\n",
    "    values\n",
    "    for k in range(K):\n",
    "        tqdm_train.update()\n",
    "        for l in range(L):\n",
    "            values = value_iteration(values, gamma)\n",
    "        q = Q(policy, values, actions_dict)\n",
    "        policy = policy_improvement(policy, q)\n",
    "    return policy\n",
    "\n",
    "def test_MDP(policy, NN, tqdm_test):\n",
    "    tqdm_test.reset()\n",
    "    runs = []\n",
    "    for _ in range(NN):\n",
    "        runs.append(run_test(policy))\n",
    "        tqdm_test.update()\n",
    "    runs = np.array(runs)\n",
    "    rewards, steps = runs[::,0], runs[::,1]\n",
    "    return rewards, steps\n",
    "\n",
    "def experiment(L, K, NN, gamma, tqdm_train, tqdm_test):\n",
    "    # gamma = 0.8\n",
    "    policy = train_mdp(L, K, gamma, tqdm_train)\n",
    "    return test_MDP(policy, NN, tqdm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6102e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(L, K, NN, gammas):\n",
    "    exps=[]\n",
    "    tqdm_gammas = tqdm.tqdm_notebook(total=len(gammas), desc='gammas', position=0)\n",
    "    tqdm_train = tqdm.tqdm_notebook(total=K, desc='train', position=1)\n",
    "    tqdm_test  = tqdm.tqdm_notebook(total=NN, desc='test', position=2)\n",
    "    for gamma in gammas:\n",
    "        tqdm_gammas.set_description('gamma:'+f\"{gamma:0.5}\")\n",
    "        rewards, steps = experiment(L, K, NN, gamma, tqdm_train, tqdm_test)\n",
    "        exps.append(stats(gamma, rewards, steps))\n",
    "        tqdm_gammas.update()\n",
    "    return np.array(exps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28971023",
   "metadata": {},
   "source": [
    "gammas = np.linspace(0.01,1, num=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ad7c9",
   "metadata": {},
   "source": [
    "NN = 5000\n",
    "L = 100\n",
    "K = 100\n",
    "exps_01_1 = experiments(L, K, NN, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfd4244",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.ylim((0,100))\n",
    "plt.scatter(exps_01_1[::,0],exps_01_1[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a76fa",
   "metadata": {},
   "source": [
    "np.mean(exps_01_1[::,1][:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f2bdf",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "# plt.ylim((0,100))\n",
    "plt.hist(exps[exps[::,3]<18][::,3], bins=100,)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33115126",
   "metadata": {},
   "source": [
    "gammas = np.linspace(0.9,0.99, num=100)\n",
    "gammas\n",
    "\n",
    "NN = 5000\n",
    "L = 100\n",
    "K = 100\n",
    "exps_short_9_99 = experiments(L, K, NN, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d361ef",
   "metadata": {},
   "source": [
    "exps_short[::,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066b1e60",
   "metadata": {},
   "source": [
    "exps_short[::,1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97500321",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.ylim((0,100))\n",
    "plt.scatter(exps_short[::,0],exps_short[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623aea0f",
   "metadata": {},
   "source": [
    "gammas = np.linspace(0.99,0.9999, num=100)\n",
    "gammas\n",
    "\n",
    "NN = 5000\n",
    "L = 100\n",
    "K = 100\n",
    "exps_short = experiments(L, K, NN, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88d095",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.ylim((0,100))\n",
    "plt.scatter(exps_short[::,0],exps_short[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268db2d8",
   "metadata": {},
   "source": [
    "gammas = np.linspace(0.998,0.99999, num=100)\n",
    "gammas\n",
    "\n",
    "NN = 5000\n",
    "L = 100\n",
    "K = 100\n",
    "exps_short2 = experiments(L, K, NN, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01105766",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.ylim((0,100))\n",
    "plt.scatter(exps_short2[::,0],exps_short2[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79ac60",
   "metadata": {},
   "source": [
    "# g, sucess, fail, sucess_len, fail_len\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.ylim((0,100))\n",
    "plt.scatter(exps_short[::,0],exps_short[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fbc303",
   "metadata": {},
   "source": [
    "gammas = np.linspace(0.988,0.999, num=100)\n",
    "gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55b5bf",
   "metadata": {},
   "source": [
    "NN = 5000\n",
    "L = 100\n",
    "K = 100\n",
    "exps_short_988_999 = experiments(L, K, NN, gammas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4447fb1",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "# plt.ylim((0,100))\n",
    "plt.scatter(exps_short_988_999[::,0],exps_short_988_999[::,1]*100, alpha=0.8)\n",
    "plt.title('The % of successfull trajectories vs Gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91eb25ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f646667bfe61462896ace14457eab367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5848d5aee196493d8cb4ed83aec0c51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     K     |     L     |   gamma   |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.736    \u001b[0m | \u001b[0m61.27    \u001b[0m | \u001b[0m15.36    \u001b[0m | \u001b[0m0.8205   \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.739    \u001b[0m | \u001b[95m94.98    \u001b[0m | \u001b[95m14.0     \u001b[0m | \u001b[95m0.8798   \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.699    \u001b[0m | \u001b[0m59.49    \u001b[0m | \u001b[0m95.37    \u001b[0m | \u001b[0m0.8519   \u001b[0m |\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m0.733    \u001b[0m | \u001b[0m4.415    \u001b[0m | \u001b[0m24.59    \u001b[0m | \u001b[0m0.9574   \u001b[0m |\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m0.737    \u001b[0m | \u001b[0m95.43    \u001b[0m | \u001b[0m87.05    \u001b[0m | \u001b[0m0.832    \u001b[0m |\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m0.759    \u001b[0m | \u001b[95m96.48    \u001b[0m | \u001b[95m13.47    \u001b[0m | \u001b[95m0.8316   \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.748    \u001b[0m | \u001b[0m97.0     \u001b[0m | \u001b[0m12.22    \u001b[0m | \u001b[0m0.8906   \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.76     \u001b[0m | \u001b[95m97.51    \u001b[0m | \u001b[95m15.05    \u001b[0m | \u001b[95m0.8046   \u001b[0m |\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m0.723    \u001b[0m | \u001b[0m99.53    \u001b[0m | \u001b[0m13.85    \u001b[0m | \u001b[0m0.8802   \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.74     \u001b[0m | \u001b[0m96.22    \u001b[0m | \u001b[0m17.15    \u001b[0m | \u001b[0m0.9064   \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.728    \u001b[0m | \u001b[0m96.55    \u001b[0m | \u001b[0m14.65    \u001b[0m | \u001b[0m0.9745   \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.743    \u001b[0m | \u001b[0m95.08    \u001b[0m | \u001b[0m14.01    \u001b[0m | \u001b[0m0.8779   \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.741    \u001b[0m | \u001b[0m96.68    \u001b[0m | \u001b[0m13.11    \u001b[0m | \u001b[0m0.8251   \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.745    \u001b[0m | \u001b[0m97.18    \u001b[0m | \u001b[0m15.21    \u001b[0m | \u001b[0m0.9125   \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.726    \u001b[0m | \u001b[0m96.0     \u001b[0m | \u001b[0m13.51    \u001b[0m | \u001b[0m0.801    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.724    \u001b[0m | \u001b[0m47.67    \u001b[0m | \u001b[0m89.93    \u001b[0m | \u001b[0m0.8064   \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.742    \u001b[0m | \u001b[0m83.26    \u001b[0m | \u001b[0m62.59    \u001b[0m | \u001b[0m0.8434   \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.694    \u001b[0m | \u001b[0m97.03    \u001b[0m | \u001b[0m13.53    \u001b[0m | \u001b[0m0.8611   \u001b[0m |\n",
      "| \u001b[0m19       \u001b[0m | \u001b[0m0.742    \u001b[0m | \u001b[0m18.23    \u001b[0m | \u001b[0m84.23    \u001b[0m | \u001b[0m0.8256   \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.751    \u001b[0m | \u001b[0m96.7     \u001b[0m | \u001b[0m13.45    \u001b[0m | \u001b[0m0.9446   \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.75     \u001b[0m | \u001b[0m97.27    \u001b[0m | \u001b[0m14.98    \u001b[0m | \u001b[0m0.8276   \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.713    \u001b[0m | \u001b[0m3.773    \u001b[0m | \u001b[0m60.15    \u001b[0m | \u001b[0m0.9386   \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.709    \u001b[0m | \u001b[0m96.07    \u001b[0m | \u001b[0m17.11    \u001b[0m | \u001b[0m0.8235   \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.745    \u001b[0m | \u001b[0m96.84    \u001b[0m | \u001b[0m12.09    \u001b[0m | \u001b[0m0.9545   \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.738    \u001b[0m | \u001b[0m97.38    \u001b[0m | \u001b[0m12.32    \u001b[0m | \u001b[0m0.8649   \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.744    \u001b[0m | \u001b[0m97.32    \u001b[0m | \u001b[0m14.99    \u001b[0m | \u001b[0m0.8904   \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.711    \u001b[0m | \u001b[0m96.44    \u001b[0m | \u001b[0m13.77    \u001b[0m | \u001b[0m0.9729   \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.727    \u001b[0m | \u001b[0m96.39    \u001b[0m | \u001b[0m17.32    \u001b[0m | \u001b[0m0.8592   \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.749    \u001b[0m | \u001b[0m78.74    \u001b[0m | \u001b[0m60.85    \u001b[0m | \u001b[0m0.9631   \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.734    \u001b[0m | \u001b[0m83.53    \u001b[0m | \u001b[0m62.54    \u001b[0m | \u001b[0m0.9521   \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.743    \u001b[0m | \u001b[0m34.46    \u001b[0m | \u001b[0m66.29    \u001b[0m | \u001b[0m0.9484   \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.759    \u001b[0m | \u001b[0m97.33    \u001b[0m | \u001b[0m14.67    \u001b[0m | \u001b[0m0.8541   \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.735    \u001b[0m | \u001b[0m78.46    \u001b[0m | \u001b[0m60.62    \u001b[0m | \u001b[0m0.9099   \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.739    \u001b[0m | \u001b[0m98.47    \u001b[0m | \u001b[0m43.46    \u001b[0m | \u001b[0m0.8779   \u001b[0m |\n",
      "| \u001b[0m35       \u001b[0m | \u001b[0m0.746    \u001b[0m | \u001b[0m96.2     \u001b[0m | \u001b[0m17.13    \u001b[0m | \u001b[0m0.9789   \u001b[0m |\n",
      "| \u001b[0m36       \u001b[0m | \u001b[0m0.723    \u001b[0m | \u001b[0m97.57    \u001b[0m | \u001b[0m14.96    \u001b[0m | \u001b[0m0.8429   \u001b[0m |\n",
      "| \u001b[0m37       \u001b[0m | \u001b[0m0.734    \u001b[0m | \u001b[0m96.82    \u001b[0m | \u001b[0m12.16    \u001b[0m | \u001b[0m0.8143   \u001b[0m |\n",
      "| \u001b[0m38       \u001b[0m | \u001b[0m0.736    \u001b[0m | \u001b[0m78.69    \u001b[0m | \u001b[0m61.01    \u001b[0m | \u001b[0m0.8148   \u001b[0m |\n",
      "| \u001b[0m39       \u001b[0m | \u001b[0m0.74     \u001b[0m | \u001b[0m38.59    \u001b[0m | \u001b[0m88.22    \u001b[0m | \u001b[0m0.8521   \u001b[0m |\n",
      "| \u001b[0m40       \u001b[0m | \u001b[0m0.734    \u001b[0m | \u001b[0m27.36    \u001b[0m | \u001b[0m86.73    \u001b[0m | \u001b[0m0.9594   \u001b[0m |\n",
      "| \u001b[0m41       \u001b[0m | \u001b[0m0.738    \u001b[0m | \u001b[0m81.82    \u001b[0m | \u001b[0m22.33    \u001b[0m | \u001b[0m0.8029   \u001b[0m |\n",
      "| \u001b[0m42       \u001b[0m | \u001b[0m0.723    \u001b[0m | \u001b[0m54.38    \u001b[0m | \u001b[0m92.81    \u001b[0m | \u001b[0m0.9403   \u001b[0m |\n",
      "| \u001b[0m43       \u001b[0m | \u001b[0m0.729    \u001b[0m | \u001b[0m97.46    \u001b[0m | \u001b[0m14.72    \u001b[0m | \u001b[0m0.8922   \u001b[0m |\n",
      "| \u001b[0m44       \u001b[0m | \u001b[0m0.739    \u001b[0m | \u001b[0m33.63    \u001b[0m | \u001b[0m61.48    \u001b[0m | \u001b[0m0.8659   \u001b[0m |\n",
      "| \u001b[0m45       \u001b[0m | \u001b[0m0.727    \u001b[0m | \u001b[0m20.43    \u001b[0m | \u001b[0m5.478    \u001b[0m | \u001b[0m0.8099   \u001b[0m |\n",
      "| \u001b[0m46       \u001b[0m | \u001b[0m0.743    \u001b[0m | \u001b[0m78.68    \u001b[0m | \u001b[0m60.71    \u001b[0m | \u001b[0m0.8181   \u001b[0m |\n",
      "| \u001b[0m47       \u001b[0m | \u001b[0m0.745    \u001b[0m | \u001b[0m61.37    \u001b[0m | \u001b[0m46.13    \u001b[0m | \u001b[0m0.9153   \u001b[0m |\n",
      "| \u001b[0m48       \u001b[0m | \u001b[0m0.745    \u001b[0m | \u001b[0m41.04    \u001b[0m | \u001b[0m6.952    \u001b[0m | \u001b[0m0.9687   \u001b[0m |\n",
      "| \u001b[0m49       \u001b[0m | \u001b[0m0.703    \u001b[0m | \u001b[0m16.62    \u001b[0m | \u001b[0m13.6     \u001b[0m | \u001b[0m0.9945   \u001b[0m |\n",
      "| \u001b[0m50       \u001b[0m | \u001b[0m0.733    \u001b[0m | \u001b[0m61.46    \u001b[0m | \u001b[0m46.1     \u001b[0m | \u001b[0m0.911    \u001b[0m |\n",
      "| \u001b[0m51       \u001b[0m | \u001b[0m0.738    \u001b[0m | \u001b[0m89.2     \u001b[0m | \u001b[0m26.48    \u001b[0m | \u001b[0m0.814    \u001b[0m |\n",
      "| \u001b[0m52       \u001b[0m | \u001b[0m0.706    \u001b[0m | \u001b[0m41.09    \u001b[0m | \u001b[0m7.115    \u001b[0m | \u001b[0m0.9904   \u001b[0m |\n",
      "| \u001b[0m53       \u001b[0m | \u001b[0m0.739    \u001b[0m | \u001b[0m96.61    \u001b[0m | \u001b[0m13.11    \u001b[0m | \u001b[0m0.9526   \u001b[0m |\n",
      "| \u001b[0m54       \u001b[0m | \u001b[0m0.727    \u001b[0m | \u001b[0m83.2     \u001b[0m | \u001b[0m62.61    \u001b[0m | \u001b[0m0.8327   \u001b[0m |\n",
      "| \u001b[0m55       \u001b[0m | \u001b[0m0.733    \u001b[0m | \u001b[0m29.2     \u001b[0m | \u001b[0m80.82    \u001b[0m | \u001b[0m0.8161   \u001b[0m |\n",
      "=============================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bc989743a290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Print the hyperparameters that resulted in the best performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "L = 100\n",
    "K = 100\n",
    "NN = 1000\n",
    "tqdm_train = tqdm.tqdm_notebook(total=K, desc='train', position=1)\n",
    "tqdm_test  = tqdm.tqdm_notebook(total=NN, desc='test', position=2)\n",
    "\n",
    "# Define the objective function for the MDP\n",
    "def objective(gamma, K, L):\n",
    "    # Train the MDP using the specified learning rate and discount factor\n",
    "    policy = train_MDP(int(L), int(K), gamma, tqdm_train)\n",
    "\n",
    "    # Evaluate the performance of the trained MDP on a validation dataset\n",
    "    rewards, steps = test_MDP(policy, NN,  tqdm_test)\n",
    "    g, sucess, fail, sucess_len, fail_len = stats(gamma, rewards, steps)\n",
    "    # Return the performance of the MDP as the objective value\n",
    "    return sucess\n",
    "\n",
    "# Define the bounds of the hyperparameters\n",
    "bounds = {'gamma': (0.8, 0.999), 'K':(1,100), 'L':(1,100)}\n",
    "\n",
    "# Initialize the Bayesian optimization algorithm\n",
    "bo = BayesianOptimization(objective, bounds)\n",
    "\n",
    "# Run the optimization process for a maximum of 50 iterations\n",
    "bo.maximize(n_iter=50)\n",
    "\n",
    "# Print the hyperparameters that resulted in the best performance\n",
    "print(bo.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79748b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f6d7c90b79437fab74a5ff6eec9e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2251d0cbb2524ee39ba1e93b4703c334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     K     |     L     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m0.7488   \u001b[0m | \u001b[0m24.85    \u001b[0m | \u001b[0m1.844    \u001b[0m |\n",
      "| \u001b[95m2        \u001b[0m | \u001b[95m0.8544   \u001b[0m | \u001b[95m84.82    \u001b[0m | \u001b[95m92.81    \u001b[0m |\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m0.8446   \u001b[0m | \u001b[0m48.5     \u001b[0m | \u001b[0m32.91    \u001b[0m |\n",
      "| \u001b[95m4        \u001b[0m | \u001b[95m0.8558   \u001b[0m | \u001b[95m72.3     \u001b[0m | \u001b[95m21.0     \u001b[0m |\n",
      "| \u001b[95m5        \u001b[0m | \u001b[95m0.8562   \u001b[0m | \u001b[95m47.0     \u001b[0m | \u001b[95m10.92    \u001b[0m |\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m0.854    \u001b[0m | \u001b[0m48.23    \u001b[0m | \u001b[0m9.875    \u001b[0m |\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m0.853    \u001b[0m | \u001b[0m79.1     \u001b[0m | \u001b[0m42.65    \u001b[0m |\n",
      "| \u001b[95m8        \u001b[0m | \u001b[95m0.8566   \u001b[0m | \u001b[95m82.25    \u001b[0m | \u001b[95m68.98    \u001b[0m |\n",
      "| \u001b[95m9        \u001b[0m | \u001b[95m0.8672   \u001b[0m | \u001b[95m63.07    \u001b[0m | \u001b[95m81.59    \u001b[0m |\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m0.8612   \u001b[0m | \u001b[0m58.36    \u001b[0m | \u001b[0m62.86    \u001b[0m |\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m0.8484   \u001b[0m | \u001b[0m41.6     \u001b[0m | \u001b[0m80.78    \u001b[0m |\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m0.8514   \u001b[0m | \u001b[0m58.31    \u001b[0m | \u001b[0m99.66    \u001b[0m |\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m0.8516   \u001b[0m | \u001b[0m99.18    \u001b[0m | \u001b[0m19.64    \u001b[0m |\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m0.7244   \u001b[0m | \u001b[0m1.808    \u001b[0m | \u001b[0m98.95    \u001b[0m |\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m0.8612   \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m52.06    \u001b[0m |\n",
      "| \u001b[0m16       \u001b[0m | \u001b[0m0.7288   \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m45.29    \u001b[0m |\n",
      "| \u001b[0m17       \u001b[0m | \u001b[0m0.8586   \u001b[0m | \u001b[0m90.08    \u001b[0m | \u001b[0m1.003    \u001b[0m |\n",
      "| \u001b[0m18       \u001b[0m | \u001b[0m0.8624   \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[95m19       \u001b[0m | \u001b[95m0.8682   \u001b[0m | \u001b[95m100.0    \u001b[0m | \u001b[95m77.92    \u001b[0m |\n",
      "| \u001b[0m20       \u001b[0m | \u001b[0m0.856    \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m1.0      \u001b[0m |\n",
      "| \u001b[0m21       \u001b[0m | \u001b[0m0.8472   \u001b[0m | \u001b[0m99.77    \u001b[0m | \u001b[0m65.49    \u001b[0m |\n",
      "| \u001b[0m22       \u001b[0m | \u001b[0m0.86     \u001b[0m | \u001b[0m100.0    \u001b[0m | \u001b[0m37.35    \u001b[0m |\n",
      "| \u001b[0m23       \u001b[0m | \u001b[0m0.8606   \u001b[0m | \u001b[0m99.24    \u001b[0m | \u001b[0m87.79    \u001b[0m |\n",
      "| \u001b[0m24       \u001b[0m | \u001b[0m0.8558   \u001b[0m | \u001b[0m73.2     \u001b[0m | \u001b[0m1.291    \u001b[0m |\n",
      "| \u001b[0m25       \u001b[0m | \u001b[0m0.8484   \u001b[0m | \u001b[0m55.41    \u001b[0m | \u001b[0m19.98    \u001b[0m |\n",
      "| \u001b[0m26       \u001b[0m | \u001b[0m0.8514   \u001b[0m | \u001b[0m41.01    \u001b[0m | \u001b[0m59.21    \u001b[0m |\n",
      "| \u001b[0m27       \u001b[0m | \u001b[0m0.8588   \u001b[0m | \u001b[0m52.78    \u001b[0m | \u001b[0m73.33    \u001b[0m |\n",
      "| \u001b[0m28       \u001b[0m | \u001b[0m0.849    \u001b[0m | \u001b[0m72.95    \u001b[0m | \u001b[0m100.0    \u001b[0m |\n",
      "| \u001b[0m29       \u001b[0m | \u001b[0m0.854    \u001b[0m | \u001b[0m75.0     \u001b[0m | \u001b[0m80.43    \u001b[0m |\n",
      "| \u001b[0m30       \u001b[0m | \u001b[0m0.8562   \u001b[0m | \u001b[0m61.12    \u001b[0m | \u001b[0m47.82    \u001b[0m |\n",
      "| \u001b[0m31       \u001b[0m | \u001b[0m0.8504   \u001b[0m | \u001b[0m84.04    \u001b[0m | \u001b[0m14.25    \u001b[0m |\n",
      "| \u001b[0m32       \u001b[0m | \u001b[0m0.8576   \u001b[0m | \u001b[0m72.18    \u001b[0m | \u001b[0m58.79    \u001b[0m |\n",
      "| \u001b[0m33       \u001b[0m | \u001b[0m0.8464   \u001b[0m | \u001b[0m54.17    \u001b[0m | \u001b[0m87.33    \u001b[0m |\n",
      "| \u001b[0m34       \u001b[0m | \u001b[0m0.8594   \u001b[0m | \u001b[0m65.76    \u001b[0m | \u001b[0m71.94    \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "L = 100\n",
    "K = 100\n",
    "NN = 5000\n",
    "tqdm_train = tqdm.tqdm_notebook(total=K, desc='train', position=1)\n",
    "tqdm_test  = tqdm.tqdm_notebook(total=NN, desc='test', position=2)\n",
    "\n",
    "# Define the objective function for the MDP\n",
    "def objective(K, L):\n",
    "    # Train the MDP using the specified learning rate and discount factor\n",
    "    policy = train_MDP(int(L), int(K),  0.9884, tqdm_train)\n",
    "\n",
    "    # Evaluate the performance of the trained MDP on a validation dataset\n",
    "    rewards, steps = test_MDP(policy, NN,  tqdm_test)\n",
    "    g, sucess, fail, sucess_len, fail_len = stats(gamma, rewards, steps)\n",
    "    # Return the performance of the MDP as the objective value\n",
    "    return sucess\n",
    "\n",
    "# Define the bounds of the hyperparameters\n",
    "bounds = {'K':(1,100), 'L':(1,100)}\n",
    "\n",
    "# Initialize the Bayesian optimization algorithm\n",
    "bo = BayesianOptimization(objective, bounds)\n",
    "\n",
    "# Run the optimization process for a maximum of 50 iterations\n",
    "bo.maximize(n_iter=50)\n",
    "\n",
    "# Print the hyperparameters that resulted in the best performance\n",
    "# print(bo.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25e1ac68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': 0.76, 'params': {'K': 97.51249788054457, 'L': 15.053673244262425, 'gamma': 0.8045601807441422}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "max_params = None\n",
    "for res in bo.res:\n",
    "    if (res['target']>t):\n",
    "        max_params = res\n",
    "        t = res['target']\n",
    "print(max_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63d00722",
   "metadata": {},
   "outputs": [],
   "source": [
    "dones = np.array([(r['target'], r['params']['gamma']) for r in bo.res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81c69df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3ff87d1410>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(dones[::,0], dones[::,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46393fec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "03131d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.0147    ,  0.9853    , 11.96598639,  6.61230082],\n",
       "       [ 0.11111111,  0.0142    ,  0.9858    , 12.92957746,  6.57892067],\n",
       "       [ 0.22222222,  0.0139    ,  0.9861    , 12.33093525,  6.65175946],\n",
       "       [ 0.33333333,  0.0157    ,  0.9843    , 12.21656051,  6.63679772],\n",
       "       [ 0.44444444,  0.0155    ,  0.9845    , 11.75483871,  6.5523616 ],\n",
       "       [ 0.55555556,  0.0167    ,  0.9833    , 13.4251497 ,  6.5096105 ],\n",
       "       [ 0.66666667,  0.0151    ,  0.9849    , 11.96688742,  6.66422987],\n",
       "       [ 0.77777778,  0.0148    ,  0.9852    , 12.38513514,  6.60535932],\n",
       "       [ 0.88888889,  0.0156    ,  0.9844    , 12.30128205,  6.51787891],\n",
       "       [ 1.        ,  0.014     ,  0.986     , 12.40714286,  6.64371197]])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fa5f7249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2d3bb130",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-224bc98e7af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "g, r, s = exps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "26618b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "2eae7719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.012, 0.988, 11.666666666666666, 6.618421052631579)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(g, r, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "58c3e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for i in range(100):\n",
    "        possible_actions = list(sorted(env.get_possible_actions(state)))\n",
    "#         print(possible_actions)\n",
    "        state_y, state_x = state\n",
    "        actions_distr = policy[state_y][state_x]\n",
    "\n",
    "        action = np.random.choice(possible_actions,p=actions_distr)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         print(state, reward, done, _,action, actions_distr)\n",
    "#         env.render()\n",
    "        total_reward += reward\n",
    "\n",
    "        if done: break\n",
    "    return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2e6c73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:06<00:00, 1459.44it/s]\n"
     ]
    }
   ],
   "source": [
    "NN = 10000\n",
    "runs = [run_test()for _ in tqdm.tqdm(range(NN))]\n",
    "runs = np.array(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "43b288f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63778228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7276 0.2724\n"
     ]
    }
   ],
   "source": [
    "success = np.sum(runs[::,0]==1.)/NN\n",
    "fail = 1-success\n",
    "print(success, fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c68acfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(runs[runs[::,0]==0.][::,1], color='red' , alpha=0.3, bins=50)\n",
    "plt.hist(runs[runs[::,0]==1.][::,1], color='blue', alpha=0.3, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd16c726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e752bec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332952b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      " 62%|██████▏   | 62/100 [00:02<00:01, 23.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c1c0c2a6ad92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_step_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_improvement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c1c0c2a6ad92>\u001b[0m in \u001b[0;36mpolicy_evaluation\u001b[0;34m(policy, gamma, evaluation_step_n)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_step_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c1c0c2a6ad92>\u001b[0m in \u001b[0;36mpolicy_evaluation_step\u001b[0;34m(policy, values, gamma)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_evaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c1c0c2a6ad92>\u001b[0m in \u001b[0;36mget_q_values\u001b[0;34m(values, gamma)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mtransition_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transition_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mnext_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/wd2_small/projects/ods_rl/hmw3/Frozen_Lake.py\u001b[0m in \u001b[0;36mget_reward\u001b[0;34m(self, state, action, next_state)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;34m\"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         assert action in self.get_possible_actions(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Frozen_Lake import FrozenLakeEnv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "env = FrozenLakeEnv()\n",
    "\n",
    "def init_policy():\n",
    "    policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        policy[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            policy[state][action] = 1 / len(env.get_possible_actions(state))\n",
    "    return policy\n",
    "\n",
    "def policy_evaluation_step(policy, values, gamma):\n",
    "    q_values = get_q_values(values, gamma)\n",
    "    \n",
    "    new_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        new_values[state] = 0\n",
    "        for action in env.get_possible_actions(state):\n",
    "            new_values[state] += policy[state][action] * q_values[state][action]\n",
    "    \n",
    "    return new_values\n",
    "\n",
    "def init_values():\n",
    "    return {state: 0 for state in env.get_all_states()}\n",
    "\n",
    "def get_q_values(values, gamma):\n",
    "    q_values = {}\n",
    "    for state in env.get_all_states():\n",
    "        q_values[state] = {}\n",
    "        for action in env.get_possible_actions(state):\n",
    "            q_values[state][action] = 0\n",
    "            for next_state in env.get_next_states(state, action):\n",
    "                reward = env.get_reward(state, action, next_state)\n",
    "                transition_prob = env.get_transition_prob(state, action, next_state)\n",
    "                next_value = values[next_state]\n",
    "                q_values[state][action] += transition_prob * (reward + gamma * next_value)\n",
    "    return q_values\n",
    "\n",
    "def policy_evaluation(policy, gamma, evaluation_step_n):\n",
    "    values = init_values()\n",
    "    for _ in range(evaluation_step_n):\n",
    "        values = policy_evaluation_step(policy, values, gamma)\n",
    "    q_values = get_q_values(values, gamma)\n",
    "    return q_values\n",
    "\n",
    "def policy_improvement(q_values):\n",
    "    new_policy = {}\n",
    "    for state in env.get_all_states():\n",
    "        new_policy[state] = {}\n",
    "        max_action = None\n",
    "        max_q_value = float('-inf')\n",
    "        for action in env.get_possible_actions(state):\n",
    "            if q_values[state][action] > max_q_value:\n",
    "                max_q_value = q_values[state][action]\n",
    "                max_action = action\n",
    "        for action in env.get_possible_actions(state):\n",
    "            new_policy[state][action] = 1 if action == max_action else 0\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "iteration_n = 100\n",
    "evaluation_step_n = 100\n",
    "gamma = 0.9\n",
    "\n",
    "policy = init_policy()\n",
    "for k in tqdm.tqdm(range(iteration_n), position=0):\n",
    "    \n",
    "    q_values = policy_evaluation(policy, gamma, evaluation_step_n)\n",
    "    policy = policy_improvement(q_values)\n",
    "    \n",
    "\n",
    "total_reward = 0\n",
    "state = env.reset()\n",
    "for _ in range(100):\n",
    "    action = np.random.choice(env.get_possible_actions(state), p=list(policy[state].values()))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    env.render()\n",
    "    \n",
    "    total_reward += reward\n",
    "    \n",
    "    if done or env.is_terminal(state):\n",
    "        break\n",
    "\n",
    "print(f'total reward = {total_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63c73e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shmalex/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "plc = np.zeros((4,4,4))\n",
    "for (y,x),a  in policy.items():\n",
    "    for k,v in a.items():\n",
    "        plc[y][x][actions_dict[k]] = v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
